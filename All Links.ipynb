{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "* http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "* https://arxiv.org/pdf/1511.07916.pdf\n",
    "\n",
    "## Attention Algorithms\n",
    "\n",
    "\n",
    "##### [Bahdanau (2014)](https://arxiv.org/pdf/1409.0473v7.pdf)\n",
    "— A really important paper that made incredible progress in Neural Machine Translation & Summarization. Most of current work in those field are based on this work\n",
    "\n",
    "- Goal:\n",
    "- Metric:\n",
    "- Accuracy:\n",
    "- Comparitive Papers:\n",
    "\n",
    "##### [Luong et al (2014)](https://arxiv.org/pdf/1508.04025.pdf) \n",
    "— Propose a new Local Attention Mechanism and a Global that is close to Bahdanau. Widely used as well.\n",
    "\n",
    "BERT [Link](https://github.com/brightmart/bert_language_understanding)\n",
    "\n",
    "##### [Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)\n",
    "\n",
    "##### [A Deep Reinforced Model for Abstractive Summarization](https://arxiv.org/abs/1705.04304) + [blogpost](https://einstein.ai/research/blog/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)\n",
    "\n",
    "* https://talbaumel.github.io/blog/attention/\n",
    "* https://distill.pub/2016/augmented-rnns/\n",
    "* https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "* https://medium.com/syncedreview/memory-attention-sequences-8522f531dd43\n",
    "* http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n",
    "\n",
    "##### [Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling](https://arxiv.org/pdf/1609.01454.pdf)\n",
    "\n",
    "## Memory Networks\n",
    "\n",
    "* Paper 1: 2014 (https://arxiv.org/pdf/1410.3916.pdf)\n",
    "* Paper 2: End to End Memory Networks\n",
    "\n",
    " - https://jhui.github.io/2017/03/15/Memory-network/\n",
    " - https://arxiv.org/abs/1503.08895\n",
    "\n",
    "## Hasing Tricks:\n",
    "* https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f\n",
    "* https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087\n",
    "* Fully understand the hashing trick  [Link](https://arxiv.org/abs/1805.08539)\n",
    "\n",
    "## Multi-Label-Classification\n",
    "[PDF Link](https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf)\n",
    "\n",
    "## Perplexity:\n",
    "[Link](https://medium.com/@aerinykim/perplexity-intuition-and-derivation-105dd481c8f3)\n",
    "\n",
    "## NLU Similarity\n",
    "[Link](https://github.com/brightmart/nlu_sim)\n",
    "\n",
    "## CNN\n",
    "\n",
    "\n",
    "## RNN\n",
    "- LSTM\n",
    "  - Vanishing Gradient , BPTT\n",
    "- A sequence to sequence encoder-decoder model  \n",
    "    - [Code Github 1](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)  [Full Repo](https://github.com/bentrevett/pytorch-seq2seq)\n",
    "\n",
    "## Word Embeddings\n",
    "[Glove](https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb)\n",
    "\n",
    "\n",
    "## Conditional Random Fields\n",
    "\n",
    "[Link](crf.ipynb)\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "- ####  Cross Entropy\n",
    "\n",
    "\n",
    "\n",
    "## Topic Modelling\n",
    "\n",
    "[Dynamic NMF](https://github.com/derekgreene/dynamic-nmf)\n",
    "\n",
    "[Probabilistic topic models by David Blei](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)\n",
    "\n",
    "[Latent Dirichlet Allocation by Blei, Ng & Jordan ](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf)\n",
    "\n",
    "## Linear Algebra\n",
    "\n",
    "- Eigen Vectors\n",
    "\n",
    "## Others\n",
    "\n",
    "- Entropy\n",
    "\n",
    "- Convex Optimization\n",
    "\n",
    "- Beam Search\n",
    "  - http://www.aclweb.org/anthology/P02-1040.pdf\n",
    "\n",
    "\n",
    "- Spelling Correction\n",
    "  - https://github.com/explosion/spaCy/issues/315#issuecomment-346194645\n",
    "  - http://norvig.com/spell-correct.html\n",
    "  \n",
    "  \n",
    "- Pytudes\n",
    "   - https://github.com/norvig/pytudes#pytudes-index-of-jupyter-ipython-notebooks\n",
    "   - https://github.com/norvig/pytudes/blob/master/ipynb/Advent%20of%20Code.ipynb\n",
    "   \n",
    "- Retrofitting\n",
    "   - https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf\n",
    "   - https://medium.com/@ayush2503/retrofitting-word-vectors-to-semantic-lexicons-3f85f4208f4f\n",
    "   \n",
    "   \n",
    "- Semantic Matching Energy\n",
    "   - https://arxiv.org/abs/1301.3485\n",
    "   \n",
    "- Remove Bias from word embedding\n",
    "    - https://arxiv.org/pdf/1607.06520.pdf\n",
    "   \n",
    "- Social Attention: Modeling Attention in Human Crowds\n",
    "\n",
    "- Trie | Autocomplete\n",
    "   - https://www.geeksforgeeks.org/auto-complete-feature-using-trie\n",
    "   - Prerequisite https://www.geeksforgeeks.org/trie-insert-and-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
